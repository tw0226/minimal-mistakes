---
published: true
layout: single
title: FlowNet
category: [Deep learning, Action Recognition, Optical Flow]
toc: true
tags: [Deep learning, Action Recognition, Optical Flow]
comments: true
toc_sticky: true
read_time: true
use_math : true

---

읽은 논문

[FlowNet: Learning Optical Flow with Convolutional Networks](https://arxiv.org/pdf/1504.06852.pdf)

[FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks](https://arxiv.org/pdf/1612.01925.pdf)

## Optical Flow

Action Recognition을 공부하다 optical flow의 중요함을 느끼고 읽게 된 논문이다.
옵티컬 플로우는 보통 두 이미지 사이에서 모션 벡터를 얻어내는 방법으로 다양한 알고리즘이 있지만 대표적으로 루카스-카나데 알고리즘을 이용하고, 모든 픽셀마다 계산하게 되는 것을 dense optical flow 라고 한다.

옵티컬 플로우를 구하는 방법에 대한 자세한 설명은 [블로그](https://paeton.tistory.com/entry/%EC%98%B5%ED%8B%B0%EC%B9%BC-%ED%94%8C%EB%A1%9C%EC%9A%B0-Optical-Flow) 을 참고하면 좋을 것 같다.

옵티컬 플로우를 구하면 아래와 같은 그림이 나오면서 움직임을 파악할 수 있다.

![Imgur](https://i.imgur.com/Y8Vz0Zx.jpg)

하지만 다음같은 그림을 본다고 생각해보자.

![Imgur](https://i.imgur.com/E7BZReA.jpg)

화살표가 너무 많아서 모션을 파악하기가 쉽지 않다.

그래서 dense optical flow같은 경우 뺵뺵하게 보이는 화살표로 인해

이미지가 오히려 가려지기 때문에 따로 벡터에 따라 변환이 이루어진다.

![Imgur](https://i.imgur.com/00AMhyT.png)

위의 사진은 벡터에 따라 색상을 표시하는 표의 예시로

만약에 왼쪽 아래로 움직였다면 초록색으로 보일 것이고,

오른쪽 위로 움직였다면 분홍색, 오른쪽 아래로 움직였다면 빨간색으로 움직일 것이다.

![Imgur](https://i.imgur.com/chSBhqm.jpg)

위의 사진은 벡터가 움직임이 작을수록 색이 옅어지고, 움직임이 클수록 색이 짙어진다는 것을 볼 수 있다.

여기까지가 옵티컬 플로우의 기본적인 지식이였다.

## FlowNet

FlowNet은 이런 Optical Flow를 CNN으로 구할 수 있지 않을까에서 시작한 논문으로, 위의 사진과 같이 입력과 비슷한 아웃풋 사이즈를 가지면서 픽셀 단위로 예측하기 때문에 segmentation과 비슷하다고 볼 수 있다.

논문에서는 Flying Chairs라는 데이터셋을 가지고 2가지 네트워크
FlowNet-S, FlowNet-C를 실험한다.

### FlowNet-S

FlowNet-S에서 S는 Simple의 약자로 말그대로 간단하게 두 장의 이미지를 포개어서 6채널(RGB(3) x 2)을 입력으로 넣어서 진행한다.

![Imgur](https://i.imgur.com/AfpRnIx.png)

### FlowNet-C

![Imgur](https://i.imgur.com/tCb5WPN.png)

FlowNet-C는 이미지를 각각 따로 받았다가 나중에 합치는 네트워크로,
이렇게 진행했을 경우 Low level 단계에서 의미있는 특징을 뽑아낼 수 있고, 나중에 합칠 때 두 이미지의 특징 벡터들을 비교하면서 새로운 특징을 뽑아낸다고 한다.

두 특징 벡터를 비교할 때 correlation layer를 만들어
feature map 사이에서 상관도를 계산한다.

상관도를 계산하는 방법은 아래의 수식과 같다.
![Imgur](https://i.imgur.com/Frz8tGq.png)

상관 관계는 내적했을 때 값이 1에 가까울 수록 상관도가 높다는 것으로 이해하면 좋을 것 같다.

수식을 좀 더 설명하자면 아래와 같은 feature Map을 f1, f2라고 하고,
x1과 x2가 중점이라고 할 때, o는 각 feature Map의 커널 크기(k)만큼 element-wise하게 진행된다. (K = 2k+1)

![Imgur](https://i.imgur.com/nlSRqp7.png)

이렇게 진행되서 f1의 한점 x1이 f2을 돌았을 때 한 점에 대한 상관도 c(${x}_1$,${x}_2$) 하나가 나오게 되고, f1의 모든 점을 돌게 되면
correlation layer의 feature Map이 하나 나오게 된다.

마찬가지로 이 연산을 feature Map c개에 대해 계산 하게 되면

c(${x}_1$,${x}_2$) = c * $K^2$ 이 되고, $K = w^2 * h^2$ 이다.

이는 backward나 forward 시에도 큰 연산이여서 논문에서는 새로운 방법을 제안한다.

![Imgur](https://i.imgur.com/hKSlHuN.png)

위의 그림과 같이 전체 2k+1 개로 커널의 너비만큼 계산하는 것은 계산량이 크기 때문에 d라는 k보다 작은 커널을 생성하여 $x_2$의 범위를  제한시키면서 진행한다. (D = 2d + 1)

이렇게 하면 $x_1$은 전체적으로 진행되고, $x_2$는 $x_1$의 중심 주변으로 진행이 되는데, 비유하자면 전체 크기(K)의 convolution 연산을 진행하는 것 보다 kernel size를 더 줄여서 부분적인 계산(D)을 진행했다고 보면 된다.

다음은 FlowNet의 Correlation layer 단계에서 21 x 21 patch로 진행한 256개의 filter visualization이다.

![Imgur](https://i.imgur.com/aiiOP1V.png)

각 필터의 픽셀들은 변위들을 의미한다. 그리고 필터의 중점마다 하얗게 되어있는데, 이는 변위가 없다는 것을 의미한다.

마지막으로 네트워크 구조에서 refinement 라는 부분이 있었는데, 이 부분은 일반적인 segmentation 모델과 같이 up-convolution을 진행하면서 concat을 진행한다.

![Imgur](https://i.imgur.com/H9IzQqF.png)

### Results

논문에서는 EPE라는 EndPoint Error, 네트워크의 예측한 끝점 값들과 GT의 끝점과의 차이로 정확도를 측정한다.

![Imgur](https://i.imgur.com/VuHLjk6.png)

아래는 논문에서 나오는 결과표로 FlowNet-S와 FlowNet-C를 비교했다.

데이터셋마다 다르지만 대체적으로 FlowNet-S가 높게 나온다.

+ft는 fine-tuning의 약자로 사전학습된 모델을 썼냐의 여부이고,

+v는 네트워크에서 up-sampling 시에 경계선이라고 생각되는 곳 내부에 스무싱을 주어 좀 더 값이 고르게 되도록 만들었다고 한다.

![Imgur](https://i.imgur.com/RScXZlL.png)

(상황에 따라 다르겠지만 +v를 주었을 때 EPE 숫자가 더 낮은 것을 볼 수 있다.)

그리고, 다음은 FlowNet-S와 FlowNet-C의 차이점으로 FlowNet-C는 두 이미지를 각각 진행한 후 합쳤기 때문에 FlowNet-S에 비해 작은  부분을 잡을 수 있지만, 변위가 크게 발생한 경우에는 FlowNet-S가 더 좋은 성능을 내는 것으로 보인다.

![Imgur](https://i.imgur.com/cDKAp6y.png)

아래는 EpicFlow 논문과 비교했을 때 EPE에러를 비교한 것으로 FlowNet의 EPE가 더 낮은 것을 볼 수 있다.

![Imgur](https://i.imgur.com/B4GK1Hf.png)

## FlowNet 2.0

[FlowNet2.0](https://arxiv.org/pdf/1612.01925.pdf) 은 당연하게도 FlowNet을 개선시킨 모델로 FlowNet 1.0보다 조금 느리지만 sub-network를 추가하면서 에러가 50%이상 감소했고, 조금 변형시키면 최대 140fps까지 가능하다고 한다.
개선시킨 점은 다음과 같다.

1. Dataset Schedules

2. Stacking Networks

3. sub-network on small motions

### DataSet Schedules

논문에서는 DataSet Schedule이 학습을 성공시키는 데 중요하다는 것을 발견했다고 한다. 논문에 쓰인 데이터셋으로 FlyingChair Dataset, Flying Things3D 두 개가 있는데, FlyingThings3D 데이터셋은 그래픽으로 랜더링시킨 데이터여서 FlyingChair 데이터셋과 다르게 3D적인 움직임과 조명효과, 다양한 물체들이 있다고 한다.

논문에서는 learning-rate schedule을 다르게 하면서 FlowNet-S, FlowNet-C를 각각 학습시켰다고 한다. 아래는 논문에서 진행한 learning-rate schedule을 시각화한 것으로 $S_{short}$는 기존 FlowNet의 방식으로, 300k까지 1e-4, 300k부터 100k마다 $\frac{1}{2}$씩 감소한다.
$S_{long}$은 논문에서 진행한 방식으로 400k까지 1e-4, 이후로 200k마다 $\frac{1}{2}$씩 감소시켜서 1.2M Iteration을 학습시켰다고 한다.
$S_{fine}$은 fine-tuning으로 학습된 $S_{long}$을 1e-5부터 재학습시켜 200K 후마다 $\frac{1}{2}$씩 감소한다.

![Imgur](https://i.imgur.com/GISRM0m.png)

그리고 아래는 그 결과로 Chairs의 $S_{short}$는 기존의 FlowNet이다. 2번쨰 행을 보면 $S_{long}$이 4.24로 $S_{short}$보다 작고, $S_{fine}$은 4.21로 더 에러가 작아진다.

mixed는 두 데이터셋을 동시에 학습시킨 경우이고, Chairs$\rightarrow$Things3D는 Chairs 학습 후 Things3D 데이터셋에 대해 fine-tuning을 진행한 것이다.

마찬가지로 FlowNet-C에서도 똑같이 검증했을 때 에러가 3.04로 더 작아지는 것을 확인할 수 있고, 에러를 보았을 때 FlowNet-C가 FlowNet-S보다 더 성능이 좋다는 것을 알 수 있다.

![Imgur](https://i.imgur.com/HBqLP4n.png)

이렇게 데이터셋과 training schedule을 조금 수정하는 것만으로도 FlowNet-S는 ~25%, FlowNet-C는 ~30% 성능의 향상을 보였다.

### Stacking Networks

Optical Flow의 SOTA 논문들은 반복적인 방법에 의존한다고 한다. 그렇다면 네트워크를 깊게 쌓는 것이 반복적으로 정제하는 데에 도움이 될 수 있을지 실험을 진행했다고 한다.

논문에서는 아래의 그림과 같이 진행한다.

![Imgur](https://i.imgur.com/DaPcgpG.png)

첫번째 네트워크는 항상  이미지 두 개를 $I_1, I_2$를 입력으로 받는다. 이후의 네트워크들도 마찬가지로 $I_1, I_2$ 를 입력으로 받지만, 이전 네트워크에서 계산된 flow $w_i$ 를 받는다.

이전 네트워크의 에러를 계산하고, 업데이트를 하기 위해 2번째 이미지 $I_2$를 $w_i$를 통해 보간하여 $\widehat{I}_{2}$를 만들고, 에러 $e_i = ||\widehat{I}_{2,i}-I_1||$ 로 계산한다.

아래는 네트워크를 쌓았을 때 어떻게 되는지 보여주는 표이다.

![Imgur](https://i.imgur.com/JX6m1tO.png)

논문에서는 해당 실험을 통해 다음의 발견을 했다고 주장한다.

차례대로 보면 Net1보다는 Net1, Net2로 쌓은 것이 좋다는 것을 볼 수 있고, Warping을 진행하는 것이 안한 것보다 에러가 더 낮다는 것을 확인할 수 있다.

논문에서는 해당 실험을 통해 다음의 발견을 했다고 주장한다.

(1) 2번째 행과 3번째 행을 비교해보면 Chair 데이터셋에서는 에러가 줄었지만, Sintel Dataset에서는 에러가 그대로인 것을 볼 수 있는데 이는 오버피팅이 된거 같다고 설명한다.

(2) warping이 포함되면, 항상 성능이 좋아진다.

(3) Net1 이후 중간 loss를 추가해주는 것이 Stacked Network에 end-to-end 학습을 진행하는데 있어서 더 도움이 된다.

(4) 제일 좋은 결과는 Net1을 고정시키고, Net2를 학습시킨 것이 제일 좋았다.

#### Stacking Multiple Diverse Networks

논문에서는 단순하게 네트워크를 반복적으로 쌓아서 성능이 향상되는지 실험을 해보았다. 결과적으로 같은 weight를 가진 Network를 쌓고, fine-tuning을 진행해도 성능이 나아지진 않았다고 한다.

또, 다양하게 네트워크를 쌓아보는 것이 성능이나 네트워크의 크기를 줄이는 데 도움이 되는지 실험을 진행하였다.

논문에 따르면, 초기 네트워크는 성능 향상에 크게 제약을 받지 않는다고 한다. 하지만 논문에서는 초기 네트워크로만 FlowNet-C를 쓰고, 이후 네트워크에서는 FlowNet-C를 쓰지 않았다고 한다.
그 이유는 FlowNet-C는 쌍둥이 구조로 입력이 제대로 처리되기엔 너무 다양하기 때문이라고 한다.

그리고 네트워크 크기에 따른 성능과 실행시간을 비교했는데 다음 그림과 같다.

![Imgur](https://i.imgur.com/wXOyOIe.png)

네트워크 크기를 $\frac{3}{8}$로 진행했을 때 좋은 절충안이 되었다고 한다.

이후로 네트워크를 작게 만든 모델과 기존 모델들을 비교했다.

![Imgur](https://i.imgur.com/v7DTNCF.png)

주목할만한 것은 FlowNet2-C는 FlowNet-C보다 50%정도까지 성능이 향상되고, FlowNet2-CSS는 FlowNet2-C보다 30%정도 성능이 향상된다고 한다.
초기에 말했던 140fps는 FlowNet2-s를 이용하면 140fps가 나올 수 있다고 한다.

![Imgur](https://i.imgur.com/WnOlGkx.png)

FlowNet2-ss의 모델 크기는 11M이고, FlowNet2-S는 38M, FlowNet2-cs는 11M, Flownet2-C는 38M이라고 한다.

마지막으로 다양한 데이터셋들에 대해 평가와 예시를 보여준다.

![Imgur](https://i.imgur.com/eKa987b.png)

![Imgur](https://i.imgur.com/70ggkcN.png)